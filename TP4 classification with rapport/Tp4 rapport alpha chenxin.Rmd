---
title: "SY19 - TP3 Rapport"
author: "Chenxin LIU"
date: "2022/10/17"
output: 
  pdf_document:
    # toc: true
    # toc_depth: 2
    number_sections: true
    df_print: kable
    highlight: tango
---



```{r setup, include=FALSE}
# Load r packages (Install if missing)


if(!require("tidyverse")) {
  install.packages("tidyverse")
  library("tidyverse")
}

if(!require("caret")) {
  install.packages("caret")
  library("caret")
}

if(!require("MASS")) { # for QDA and LDA
  install.packages("MASS")
  library("MASS")
}

if(!require("naivebayes")) { # For naive bayes
  install.packages("naivebayes")
  library("naivebayes")
}

if(!require("caret")) {
  install.packages("caret")
  library("caret")
}


if(!require("FNN")) {
  install.packages("FNN")
  library("FNN")
}


if(!require("naivebayes")) { # For naive bayes
  install.packages("naivebayes")
  library("naivebayes")
}

if(!require("nnet")) { # For naive bayes
  install.packages("nnet")
  library("nnet")
}

# basic statistics
source("scripts/clas-general-import_data.R")

```

# Classifieur

##  Preparation : Partitioning raw data to train & test##

```{r}
set.seed(69)
clas.test.id <- createDataPartition(TPN1_a22_clas_app$y,
                                    p = 1/5,
                                    list = TRUE)

clas.data.test  <- TPN1_a22_clas_app[ clas.test.id[[1]],]
clas.data.train <- TPN1_a22_clas_app[-clas.test.id[[1]],]
clas.levels <- levels(TPN1_a22_clas_app$y)
```
## Data exploration

```{r echo=FALSE, out.width="30%"}
barplot(table(TPN1_a22_clas_app$y))

tmp <- max(table(TPN1_a22_clas_app$y)/nrow(TPN1_a22_clas_app))
1-tmp 
```
First we explore the data a little, there is barplot can be seen: Y consists of three classes, the number of class1 is significantly smaller than the number of class2, 3. So if we do not do machine learning and choose the class with the largest proportion each time, our error rate will be 0.58, which will be the highest error rate we can accept

## Nonparametric method kNN
The first method we choose is the non-parametric one, the knn method. Firstly we apply KNN with an arbitrary k = 10 to have a look at general result.

```{r KNN, echo=FALSE}
clas.knn.k <- 10

# Fit model
clas.knn.fit <- FNN::knn(train = scale(clas.data.train[,1:50]),
                         test  = scale(clas.data.test [,1:50]),
                         cl    = clas.data.train$y,
                         k     = clas.knn.k)

# Prediction 
# For KNN, the prediction is performed in `knn` function

# correct 'levels' of predicted classes
levels(clas.knn.fit) <- clas.levels

print("Contingency matrix:")
clas.knn.perf <- table(clas.data.test$y, clas.knn.fit)
clas.knn.perf

# Evaluate the prediction accuracy (on test data)

print("Error total:") 
clas.knn.error_total   <- 1 - sum (diag(clas.knn.perf)) / nrow(clas.data.test)
print(clas.knn.error_total, digits = 2)

print("Error within each class")
clas.knn.error_within_class <- 1 - diag(clas.knn.perf) / rowSums(clas.knn.perf)
clas.knn.error_within_class

```


The error rate reaches 0.51


### KNN with an arbitrary k
Next we try to iterate over k to see if we can optimize the error rate
```{r echo=FALSE, message=FALSE, out.width="50%"}
source("scripts/f_clas_perf.R")
source("scripts/f_clas_knn.R")

clas.knn.k_range <- 1:50
clas.knn.n_parameters <- 1

clas.test.errors <- matrix(data = NA,
                           ncol = 4 + clas.knn.n_parameters,
                           nrow = length(clas.knn.k_range))

clas.test.errors <- as.data.frame(clas.test.errors)
colnames(clas.test.errors) <- c('Err.tot', 'Err.1', 'Err.2', 'Err.3', 'param.k')

# loop : fit model to different model parameter(s)
for(i in 1:length(clas.knn.k_range)){
  
  tmp.param <- list(k = clas.knn.k_range[i])
  tmp.fit <- f_clas_knn(train = clas.data.train, 
                        test  = clas.data.test, 
                        parameters = tmp.param)
  
  clas.test.errors[i, ] <- c(tmp.fit$test.error.total,
                             tmp.fit$test.error.within_class,
                             unlist(tmp.param))
}

# Analyse Result

#head(clas.test.errors)

ggplot(data = clas.test.errors, 
       mapping = aes(x = param.k)) +
  geom_line(aes(y = Err.tot, color = "Test Err (Total)")) + 
  geom_line(aes(y = Err.1, color = "Test Err in class 1"), linetype = "dashed", alpha = 0.8) +
  geom_line(aes(y = Err.2, color = "Test Err in class 2"), linetype = "dashed", alpha = 0.8) +
  geom_line(aes(y = Err.3, color = "Test Err in class 3"), linetype = "dashed", alpha = 0.8)



#Result err min
print("Error minimal:") 
clas.test.errors[which.min(clas.test.errors$Err.tot),]

```
Observing the plot, we see that class1 has an error rate of 1 when the value of k exceeds 10, most likely because class1 is a smaller class and is therefore divided into other classes when the value of k increases. But at k = 43 we observed a minimum error rate of 0.35, which is unlikily and next we applied cross comparisons to confirm the results. 
### k_f-fold validation with k=5
In the k-fold validation, firtly we choose k = 5
```{r KNN CV, echo=FALSE, message=FALSE, warning=FALSE, out.width="50%"}

#import function
source("scripts/f_clas_knn.R")
source("scripts/f_clas_cv.R")


set.seed(69)
{# repeat CV and observe !
  clas.cv.k_folds <- 5
  tmp.cv.test.fold_id <- createFolds(y = TPN1_a22_clas_app$y, 
                                     k = clas.cv.k_folds,
                                     list = FALSE)
  
  # init a matrix (data.frame) to store the result
  tmp.k_range <- 1:100
  tmp.grid <- expand.grid(param.k = tmp.k_range, 
                          test.fold_id = 1:clas.cv.k_folds)
  
  tmp.errors <- matrix(data = NA,
                       ncol = 4,
                       nrow = nrow(tmp.grid))
  tmp.errors <- as.data.frame(tmp.errors)
  colnames(tmp.errors) [1:4] <- c('Err.tot', 'Err.1', 'Err.2', 'Err.3')
  clas.cv.test.errors <- cbind(tmp.errors, tmp.grid)
  
  # loop : fit model to different model parameter(s) in CV
  for(i in 1:nrow(clas.cv.test.errors)){
    tmp.fold_id <-          clas.cv.test.errors$test.fold_id[i]
    tmp.param   <- list(k = clas.cv.test.errors$param.k[i])
    
    tmp.data.test  <- TPN1_a22_clas_app[tmp.cv.test.fold_id == tmp.fold_id,]
    tmp.data.train <- TPN1_a22_clas_app[tmp.cv.test.fold_id != tmp.fold_id,]
    
    # loop : fit model to different model parameter(s)
    tmp.fit <- f_clas_knn(train = tmp.data.train,
                          test  = tmp.data.test,
                          parameters = tmp.param)
      
    clas.cv.test.errors[i, 1:4] <- c(tmp.fit$test.error.total,
                                     tmp.fit$test.error.within_class)
  }
  
  print("result")
  head(clas.cv.test.errors)
  
  print("best parameter k : ")
  clas.cv.test.errors %>% 
    group_by(param.k) %>% 
    summarise(avg.Err.tot = mean(Err.tot)) %>% 
    filter(avg.Err.tot == min(avg.Err.tot))
}

clas.cv.test.errors %>% 
  ggplot(mapping = aes(x = param.k, group = test.fold_id)) +
  xlab("#Neighbours") + ylab("Error rate") +
  geom_line(aes(y = Err.tot, color = "Test Err (Total)")) + 
  geom_line(aes(y = Err.1  , color = "Test Err in class 1"), linetype = "dashed", alpha = 0.5) +
  geom_line(aes(y = Err.2  , color = "Test Err in class 2"), linetype = "dashed", alpha = 0.5) +
  geom_line(aes(y = Err.3  , color = "Test Err in class 3"), linetype = "dashed", alpha = 0.5) +
  theme_light()

clas.cv.test.errors %>% 
  group_by(param.k) %>% 
  summarise(avg.Err.tot = mean(Err.tot),
            avg.Err.1   = mean(Err.1),
            avg.Err.2   = mean(Err.2),
            avg.Err.3   = mean(Err.3)) %>% 
  ggplot(mapping = aes(x = param.k)) +
  ggtitle(paste0("Plot of Average error in ", clas.cv.k_folds, "-fold CV")) +
  xlab("#Neighbours") + ylab("Avg Error rate (Cross-Validation)") +
  geom_line(aes(y = avg.Err.tot , color = "Test Err (Total)")) + 
  geom_line(aes(y = avg.Err.1   , color = "Test Err in class 1"), linetype = "dashed", alpha = 1) +
  geom_line(aes(y = avg.Err.2   , color = "Test Err in class 2"), linetype = "dashed", alpha = 1) +
  geom_line(aes(y = avg.Err.3   , color = "Test Err in class 3"), linetype = "dashed", alpha = 1) +
  theme_light()

```
After k-fold =5, we obtained optimal results at KNN k=11 with an error rate of 0.41.

### k_f-fold validation with k=10

```{r KNN CV10, echo=FALSE, message=FALSE, warning=FALSE, out.width="50%"}

#import function
source("scripts/f_clas_knn.R")
source("scripts/f_clas_cv.R")


set.seed(69)
{# repeat CV and observe !
  clas.cv.k_folds <- 10
  tmp.cv.test.fold_id <- createFolds(y = TPN1_a22_clas_app$y, 
                                     k = clas.cv.k_folds,
                                     list = FALSE)
  
  # init a matrix (data.frame) to store the result
  tmp.k_range <- 1:100
  tmp.grid <- expand.grid(param.k = tmp.k_range, 
                          test.fold_id = 1:clas.cv.k_folds)
  
  tmp.errors <- matrix(data = NA,
                       ncol = 4,
                       nrow = nrow(tmp.grid))
  tmp.errors <- as.data.frame(tmp.errors)
  colnames(tmp.errors) [1:4] <- c('Err.tot', 'Err.1', 'Err.2', 'Err.3')
  clas.cv.test.errors <- cbind(tmp.errors, tmp.grid)
  
  # loop : fit model to different model parameter(s) in CV
  for(i in 1:nrow(clas.cv.test.errors)){
    tmp.fold_id <-          clas.cv.test.errors$test.fold_id[i]
    tmp.param   <- list(k = clas.cv.test.errors$param.k[i])
    
    tmp.data.test  <- TPN1_a22_clas_app[tmp.cv.test.fold_id == tmp.fold_id,]
    tmp.data.train <- TPN1_a22_clas_app[tmp.cv.test.fold_id != tmp.fold_id,]
    
    # loop : fit model to different model parameter(s)
    tmp.fit <- f_clas_knn(train = tmp.data.train,
                          test  = tmp.data.test,
                          parameters = tmp.param)
      
    clas.cv.test.errors[i, 1:4] <- c(tmp.fit$test.error.total,
                                     tmp.fit$test.error.within_class)
  }
  
  print("result")
  head(clas.cv.test.errors)
  
  print("best parameter k : ")
  clas.cv.test.errors %>% 
    group_by(param.k) %>% 
    summarise(avg.Err.tot = mean(Err.tot)) %>% 
    filter(avg.Err.tot == min(avg.Err.tot))
}

clas.cv.test.errors %>% 
  ggplot(mapping = aes(x = param.k, group = test.fold_id)) +
  xlab("#Neighbours") + ylab("Error rate") +
  geom_line(aes(y = Err.tot, color = "Test Err (Total)")) + 
  geom_line(aes(y = Err.1  , color = "Test Err in class 1"), linetype = "dashed", alpha = 0.5) +
  geom_line(aes(y = Err.2  , color = "Test Err in class 2"), linetype = "dashed", alpha = 0.5) +
  geom_line(aes(y = Err.3  , color = "Test Err in class 3"), linetype = "dashed", alpha = 0.5) +
  theme_light()

clas.cv.test.errors %>% 
  group_by(param.k) %>% 
  summarise(avg.Err.tot = mean(Err.tot),
            avg.Err.1   = mean(Err.1),
            avg.Err.2   = mean(Err.2),
            avg.Err.3   = mean(Err.3)) %>% 
  ggplot(mapping = aes(x = param.k)) +
  ggtitle(paste0("Plot of Average error in ", clas.cv.k_folds, "-fold CV")) +
  xlab("#Neighbours") + ylab("Avg Error rate (Cross-Validation)") +
  geom_line(aes(y = avg.Err.tot , color = "Test Err (Total)")) + 
  geom_line(aes(y = avg.Err.1   , color = "Test Err in class 1"), linetype = "dashed", alpha = 1) +
  geom_line(aes(y = avg.Err.2   , color = "Test Err in class 2"), linetype = "dashed", alpha = 1) +
  geom_line(aes(y = avg.Err.3   , color = "Test Err in class 3"), linetype = "dashed", alpha = 1) +
  theme_light()

```
After k-fold =10, we obtained optimal results at KNN k=15 with an error rate of 0.41, The results are similar


## QDA

```{r message=FALSE, warning=FALSE,echo=FALSE}
source("scripts/f_clas_qda.R")

f_clas_qda(train = clas.data.train, test = clas.data.test)

```
After one QDA, we obtained an error rate of 0.38, which is already better than the optimal KNN result.

## QDA with K-fold validation
### k =5, repeats 10

```{r message=FALSE, warning=FALSE,echo=FALSE}
clas_qda.cv_repeat5         <- f_clas_cv_repeat(f_classifier = f_clas_qda        , train = TPN1_a22_clas_app, k_folds = 5, repeats = 10)
clas_qda.cv_repeat5[,1:4] %>% summary()
```
### k =10, repeats 10
```{r message=FALSE, warning=FALSE,echo=FALSE}
clas_qda.cv_repeat10         <- f_clas_cv_repeat(f_classifier = f_clas_qda        , train = TPN1_a22_clas_app, k_folds = 10, repeats = 10)
clas_qda.cv_repeat10[,1:4] %>% summary()
```

## LDA

```{r message=FALSE, warning=FALSE,echo=FALSE}
source("scripts/f_clas_lda.R")
f_clas_lda(train = clas.data.train, test = clas.data.test)

```


## LDA with K-fold validation
### k =5, repeats 10

```{r message=FALSE, warning=FALSE,echo=FALSE}
clas_lda.cv_repeat5         <- f_clas_cv_repeat(f_classifier = f_clas_lda        , train = TPN1_a22_clas_app, k_folds = 5, repeats = 10)
clas_lda.cv_repeat5[,1:4] %>% summary()
```
### k =10, repeats 10
```{r message=FALSE, warning=FALSE,echo=FALSE}
clas_lda.cv_repeat10         <- f_clas_cv_repeat(f_classifier = f_clas_qda        , train = TPN1_a22_clas_app, k_folds = 10, repeats = 10)
clas_lda.cv_repeat10[,1:4] %>% summary()
```




## Naive Bayes

```{r message=FALSE, warning=FALSE,echo=FALSE}
source("scripts/f_clas_naive_bayes.r")
f_clas_naive_bayes(train = clas.data.train, test = clas.data.test)

```


## Naive Bayes with K-fold validation
### k =5, repeats 10

```{r message=FALSE, warning=FALSE,echo=FALSE}
clas_naive_bayes.cv_repeat5         <- f_clas_cv_repeat(f_classifier = f_clas_naive_bayes        , train = TPN1_a22_clas_app, k_folds = 5, repeats = 10)
clas_naive_bayes.cv_repeat5[,1:4] %>% summary()
```
### k =10, repeats 10
```{r message=FALSE, warning=FALSE,echo=FALSE}
clas_naive_bayes.cv_repeat10         <- f_clas_cv_repeat(f_classifier = f_clas_naive_bayes        , train = TPN1_a22_clas_app, k_folds = 10, repeats = 10)
clas_naive_bayes.cv_repeat10[,1:4] %>% summary()
```




## Multinomial logistic regression

Here, our data have the classes c > 2, so we used the "Multinomial logistic regression" method



```{r message=FALSE, warning=FALSE,echo=FALSE}
source("scripts/f_clas_mulit_logistic.r")
f_clas_mulit_logistic(train = clas.data.train, test = clas.data.test)

```


## Naive Bayes with K-fold validation
### k =5, repeats 10

```{r include=FALSE,echo=FALSE}
clas_mulit_logistic.cv_repeat5 <- f_clas_cv_repeat(f_classifier = f_clas_mulit_logistic, train = TPN1_a22_clas_app, k_folds = 5, repeats = 10)
```

```{r message=FALSE, warning=FALSE,echo=FALSE}

clas_mulit_logistic.cv_repeat5[,1:4] %>% summary()
```
### k =10, repeats 10


```{r include=FALSE,echo=FALSE}
clas_mulit_logistic.cv_repeat10 <- f_clas_cv_repeat(f_classifier = f_clas_mulit_logistic, train = TPN1_a22_clas_app, k_folds = 10, repeats = 10)

```

```{r message=FALSE, warning=FALSE,echo=FALSE}

clas_mulit_logistic.cv_repeat10[,1:4] %>% summary()
```

# Principal component analysis

```{r , out.width="50%",echo=FALSE}
X <- TPN1_a22_clas_app[1:50]
X<-scale(X)
pca<-princomp(X)
Z<-pca$scores
lambda<-pca$sdev^2
plot(cumsum(lambda)/sum(lambda),type="l",xlab="q",ylab="proportion of explained variance")
```



```{r , out.width="50%",echo=FALSE}

Err.summary <- data.frame(KNN = clas.cv.test.errors$Err.tot,LDA = clas_lda.cv_repeat5$Err.tot, QDA = clas_qda.cv_repeat5$Err.tot, Naive_bayse = clas_naive_bayes.cv_repeat5$Err.tot, Logistic = clas_mulit_logistic.cv_repeat5$Err.tot)



boxplot(Err.summary,ylab="Error rate" , xlab="- Method -")




```




















