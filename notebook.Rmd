---
title: "TP4 - SY19"
authors: "Pengyu JI, Antoine Kryus, Chenxi Liu"
subtitle: "Rapport du TP4: Régression et classification - Sélection de modèles"
output:
  pdf_document:
    # toc: true
    # toc_depth: 2
    number_sections: true
    df_print: kable
    highlight: tango
---

# Regression data set

## data import, exploration and partitioning


```{r}
library(MASS)
reg.set <- read.table('data/TPN1_a22_reg_app.txt', header = TRUE)

# exploration
boxplot(as.data.frame(reg.set[1:100]))

train.percentage <- 2/3 #train partitioning
n_reg <- nrow(reg.set)
n_train <- as.integer(n_reg * train.percentage)
n_test <- n_reg - n_train
set.seed(69)
id_train <- sample(n_reg, n_train)
data.train <- reg.set[id_train,]
data.test <- reg.set[-id_train,]
y.test <- reg.set[-id_train, c(101)]
y.train <- reg.set[id_train, c(101)]
```

## Model selection

### PCA

```{r}
library(pls)
pcr_model <- pcr(y~., data = data.test, validation = "CV")
validationplot(pcr_model, val.type="MSEP")
```

According to the graph, we see that we need all variables to have the best model.

### Modèle linéaire

```{r eval=FALSE}
reg.set.lm <- lm(formula = y ~., data = data.train)
summary(reg.set.lm) # We have  small p-value and a relatively big R-Square
res_std <- rstandard(reg.set.lm)
plot(x = y.train, y = res_std) # we observe 
abline(0, 0)
pre.lm <- predict(reg.set.lm, newdata = data.test)
mse.lm <- mean((pre.lm - y.test) ^ 2)#200.176
# We do the regression again with significative
reg.set.lm.revise <- lm(formula = y~X6+X11+X12+X15+X17+X22+X23+X25+X27+X32+X33+X35+X37+X39+X46+X47+X48+X49+X52+X54+X56+X59+X60+X63+X68+X70+X72+X74+X79+X83+X84+X87+X88+X89+X90+X91+X96
                        , data = data.train)
pre.lm.revise <- predict(reg.set.lm.revise, newdata = data.test)
mse.lm1 <- mean((pre.lm.revise - y.test) ^ 2)#200.176
```

We got a R² of 0.9626 in the linear model with a really small p-value (\< 2.2e-16). We got a mean squared error (MSE) of 200.176.

Doing again the linear regression we got a R² of 0.9429 and a MSE of 246.1544, which is a bit worse, as predicted with the PCA analysis (it is better to keep all coefficient).

## Subset selection

We even though still want to try with the Backward and Forward selection method what we have in we regress our data in a model.

```{r eval=FALSE}
library(leaps)
library(dplyr)
reg.selection.forward <- regsubsets(y~., data = data.train, method = "forward", nbest = 1, nvmax = 100)
summary_forward <- summary(reg.selection.forward)
plot(reg.selection.forward, scale = "bic")#Regarder bri??vement la plus grande adjusted R Square

rss<-data.frame(summary_forward$outmat, RSS=summary_forward$rss)
rsquare_max_forward <- summary_forward$outmat[which.max(summary_forward$adjr2),]#La ligne avec la plus grande adjr2
rsquare_max_forward[rsquare_max_forward == '*'] <- as.numeric(1)
rsquare_max_forward[rsquare_max_forward == ' '] <- as.numeric(0)
rsquare_max_forward <- as.numeric(rsquare_max_forward)#Le masque pour s??lectionner les variables
reg.subset.forward <- reg.set[c(rsquare_max_forward==1)]


n.subset.forward <- nrow(reg.subset.forward)
set.seed(69)
n.subset.forward.train <- as.integer(train.percentage * n.subset.forward)
n.subset.forward.sample <- sample(n.subset.forward, n.subset.forward.train)
reg.subset.forward.train <- reg.subset.forward[n.subset.forward.sample,]
reg.subset.forward.test <- reg.subset.forward[-n.subset.forward.sample,]
reg.subset.forward.lm <- lm(formula = y~., data = reg.subset.forward.train)
reg.subset.forward.lm.predict <- predict(reg.subset.forward.lm, newdata = reg.subset.forward.test)
reg.subset.forward.mse <- mean((reg.subset.forward.lm.predict - reg.subset.forward.test$y) ^ 2) #MSE = 188.44
reg.subset.forward.err <- rstandard(reg.subset.forward.lm)
plot(reg.subset.forward.train$y, reg.subset.forward.err)
abline(0, 0)
```

We did the same for Backward selection. We then have

# Classification data set

```{r setup, include=FALSE}
# Load r packages (Install if missing)

if(!require("tidyverse")) {
  install.packages("tidyverse")
  library("tidyverse")
}

if(!require("caret")) {
  install.packages("caret")
  library("caret")
}

if(!require("MASS")) { # for QDA and LDA
  install.packages("MASS")
  library("MASS")
}

if(!require("naivebayes")) { # For naive bayes
  install.packages("naivebayes")
  library("naivebayes")
}

if(!require("caret")) {
  install.packages("caret")
  library("caret")
}


if(!require("FNN")) {
  install.packages("FNN")
  library("FNN")
}


if(!require("naivebayes")) { # For naive bayes
  install.packages("naivebayes")
  library("naivebayes")
}

if(!require("nnet")) { # For naive bayes
  install.packages("nnet")
  library("nnet")
}

# basic statistics
source("scripts/clas-general-import_data.R")

```

# Classification

## Preparation : Partitioning raw data to train & test

```{r echo=FALSE}
set.seed(69)
clas.test.id <- createDataPartition(TPN1_a22_clas_app$y,
                                    p = 1/5,
                                    list = TRUE)

clas.data.test  <- TPN1_a22_clas_app[ clas.test.id[[1]],]
clas.data.train <- TPN1_a22_clas_app[-clas.test.id[[1]],]
clas.levels <- levels(TPN1_a22_clas_app$y)
```

In order to separate the training and test data, we chose to randomly shuffle the data, and to take as many as four fifths of them for training. We also tested with two thirds of the training data, but the errors were slightly higher.

## Data exploration

```{r echo=FALSE, out.width="30%"}
barplot(table(TPN1_a22_clas_app$y))

tmp <- max(table(TPN1_a22_clas_app$y)/nrow(TPN1_a22_clas_app))
1-tmp 
```

We explore the data a with barplot can be seen: Y consists of three classes, the number of class1 is significantly smaller than the number of class2, 3. So if we do not do machine learning and choose the class with the largest proportion each time, our error rate will be 0.58, which will be the highest error rate we can accept

## Nonparametric method: kNN

The first method we choose is the non-parametric one, the KNN method. Firstly we apply KNN with an arbitrary k = 10 to have a look at general result.

```{r KNN1, echo=FALSE}
clas.knn.k <- 10

# Fit model
clas.knn.fit <- FNN::knn(train = scale(clas.data.train[,1:50]),
                         test  = scale(clas.data.test [,1:50]),
                         cl    = clas.data.train$y,
                         k     = clas.knn.k)

# Prediction 
# For KNN, the prediction is performed in `knn` function

# correct 'levels' of predicted classes
levels(clas.knn.fit) <- clas.levels

#print("Contingency matrix:")
clas.knn.perf <- table(clas.data.test$y, clas.knn.fit)
#clas.knn.perf

# Evaluate the prediction accuracy (on test data)

print("Error total:") 
clas.knn.error_total   <- 1 - sum (diag(clas.knn.perf)) / nrow(clas.data.test)
print(clas.knn.error_total, digits = 2)

#print("Error within each class")
```

Error within each class:

```{r KNN2, echo=FALSE}

clas.knn.error_within_class <- 1 - diag(clas.knn.perf) / rowSums(clas.knn.perf)
clas.knn.error_within_class

```

The error rate reaches 0.51

### KNN with an arbitrary k

Next we try to iterate over k to see if we can optimize the error rate

```{r echo=FALSE, message=FALSE, out.width="50%"}
source("scripts/f_clas_perf.R")
source("scripts/f_clas_knn.R")

clas.knn.k_range <- 1:50
clas.knn.n_parameters <- 1

clas.test.errors <- matrix(data = NA,
                           ncol = 4 + clas.knn.n_parameters,
                           nrow = length(clas.knn.k_range))

clas.test.errors <- as.data.frame(clas.test.errors)
colnames(clas.test.errors) <- c('Err.tot', 'Err.1', 'Err.2', 'Err.3', 'param.k')

# loop : fit model to different model parameter(s)
for(i in 1:length(clas.knn.k_range)){
  
  tmp.param <- list(k = clas.knn.k_range[i])
  tmp.fit <- f_clas_knn(train = clas.data.train, 
                        test  = clas.data.test, 
                        parameters = tmp.param)
  
  clas.test.errors[i, ] <- c(tmp.fit$test.error.total,
                             tmp.fit$test.error.within_class,
                             unlist(tmp.param))
}

# Analyse Result

#head(clas.test.errors)

ggplot(data = clas.test.errors, 
       mapping = aes(x = param.k)) +
  geom_line(aes(y = Err.tot, color = "Test Err (Total)")) + 
  geom_line(aes(y = Err.1, color = "Test Err in class 1"), linetype = "dashed", alpha = 0.8) +
  geom_line(aes(y = Err.2, color = "Test Err in class 2"), linetype = "dashed", alpha = 0.8) +
  geom_line(aes(y = Err.3, color = "Test Err in class 3"), linetype = "dashed", alpha = 0.8)



#Result err min
print("Error minimal:") 
clas.test.errors[which.min(clas.test.errors$Err.tot),]

```

Observing the plot, we see that class1 has an error rate of 1 when the value of k exceeds 10, most likely because class1 is a smaller class and is therefore divided into other classes when the value of k increases. But at k = 43 we observed a minimum error rate of 0.35, which is unlikily and next we applied cross comparisons to confirm the results. \### k_f-fold validation with k=5 In the k-fold validation, firtly we choose k = 5

```{r KNN CV, echo=FALSE, message=FALSE, warning=FALSE, out.width="50%"}

#import function
source("scripts/f_clas_knn.R")
source("scripts/f_clas_cv.R")


set.seed(69)
{# repeat CV and observe !
  clas.cv.k_folds <- 5
  tmp.cv.test.fold_id <- createFolds(y = TPN1_a22_clas_app$y, 
                                     k = clas.cv.k_folds,
                                     list = FALSE)
  
  # init a matrix (data.frame) to store the result
  tmp.k_range <- 1:100
  tmp.grid <- expand.grid(param.k = tmp.k_range, 
                          test.fold_id = 1:clas.cv.k_folds)
  
  tmp.errors <- matrix(data = NA,
                       ncol = 4,
                       nrow = nrow(tmp.grid))
  tmp.errors <- as.data.frame(tmp.errors)
  colnames(tmp.errors) [1:4] <- c('Err.tot', 'Err.1', 'Err.2', 'Err.3')
  clas.cv.test.errors <- cbind(tmp.errors, tmp.grid)
  
  # loop : fit model to different model parameter(s) in CV
  for(i in 1:nrow(clas.cv.test.errors)){
    tmp.fold_id <-          clas.cv.test.errors$test.fold_id[i]
    tmp.param   <- list(k = clas.cv.test.errors$param.k[i])
    
    tmp.data.test  <- TPN1_a22_clas_app[tmp.cv.test.fold_id == tmp.fold_id,]
    tmp.data.train <- TPN1_a22_clas_app[tmp.cv.test.fold_id != tmp.fold_id,]
    
    # loop : fit model to different model parameter(s)
    tmp.fit <- f_clas_knn(train = tmp.data.train,
                          test  = tmp.data.test,
                          parameters = tmp.param)
      
    clas.cv.test.errors[i, 1:4] <- c(tmp.fit$test.error.total,
                                     tmp.fit$test.error.within_class)
  }
  
  print("result")
  head(clas.cv.test.errors)
  
  print("best parameter k : ")
  clas.cv.test.errors %>% 
    group_by(param.k) %>% 
    summarise(avg.Err.tot = mean(Err.tot)) %>% 
    filter(avg.Err.tot == min(avg.Err.tot))
}

clas.cv.test.errors %>% 
  ggplot(mapping = aes(x = param.k, group = test.fold_id)) +
  xlab("#Neighbours") + ylab("Error rate") +
  geom_line(aes(y = Err.tot, color = "Test Err (Total)")) + 
  geom_line(aes(y = Err.1  , color = "Test Err in class 1"), linetype = "dashed", alpha = 0.5) +
  geom_line(aes(y = Err.2  , color = "Test Err in class 2"), linetype = "dashed", alpha = 0.5) +
  geom_line(aes(y = Err.3  , color = "Test Err in class 3"), linetype = "dashed", alpha = 0.5) +
  theme_light()

clas.cv.test.errors %>% 
  group_by(param.k) %>% 
  summarise(avg.Err.tot = mean(Err.tot),
            avg.Err.1   = mean(Err.1),
            avg.Err.2   = mean(Err.2),
            avg.Err.3   = mean(Err.3)) %>% 
  ggplot(mapping = aes(x = param.k)) +
  ggtitle(paste0("Plot of Average error in ", clas.cv.k_folds, "-fold CV")) +
  xlab("#Neighbours") + ylab("Avg Error rate (Cross-Validation)") +
  geom_line(aes(y = avg.Err.tot , color = "Test Err (Total)")) + 
  geom_line(aes(y = avg.Err.1   , color = "Test Err in class 1"), linetype = "dashed", alpha = 1) +
  geom_line(aes(y = avg.Err.2   , color = "Test Err in class 2"), linetype = "dashed", alpha = 1) +
  geom_line(aes(y = avg.Err.3   , color = "Test Err in class 3"), linetype = "dashed", alpha = 1) +
  theme_light()

```

After k-fold =5, we obtained optimal results at KNN k=11 with an error rate of 0.41.

### k_f-fold validation with k=10

```{r KNN CV10, echo=FALSE, message=FALSE, warning=FALSE, out.width="50%"}

#import function
source("scripts/f_clas_knn.R")
source("scripts/f_clas_cv.R")


set.seed(69)
{# repeat CV and observe !
  clas.cv.k_folds <- 10
  tmp.cv.test.fold_id <- createFolds(y = TPN1_a22_clas_app$y, 
                                     k = clas.cv.k_folds,
                                     list = FALSE)
  
  # init a matrix (data.frame) to store the result
  tmp.k_range <- 1:100
  tmp.grid <- expand.grid(param.k = tmp.k_range, 
                          test.fold_id = 1:clas.cv.k_folds)
  
  tmp.errors <- matrix(data = NA,
                       ncol = 4,
                       nrow = nrow(tmp.grid))
  tmp.errors <- as.data.frame(tmp.errors)
  colnames(tmp.errors) [1:4] <- c('Err.tot', 'Err.1', 'Err.2', 'Err.3')
  clas.cv.test.errors <- cbind(tmp.errors, tmp.grid)
  
  # loop : fit model to different model parameter(s) in CV
  for(i in 1:nrow(clas.cv.test.errors)){
    tmp.fold_id <-          clas.cv.test.errors$test.fold_id[i]
    tmp.param   <- list(k = clas.cv.test.errors$param.k[i])
    
    tmp.data.test  <- TPN1_a22_clas_app[tmp.cv.test.fold_id == tmp.fold_id,]
    tmp.data.train <- TPN1_a22_clas_app[tmp.cv.test.fold_id != tmp.fold_id,]
    
    # loop : fit model to different model parameter(s)
    tmp.fit <- f_clas_knn(train = tmp.data.train,
                          test  = tmp.data.test,
                          parameters = tmp.param)
      
    clas.cv.test.errors[i, 1:4] <- c(tmp.fit$test.error.total,
                                     tmp.fit$test.error.within_class)
  }
  
  print("result")
  head(clas.cv.test.errors)
  
  print("best parameter k : ")
  clas.cv.test.errors %>% 
    group_by(param.k) %>% 
    summarise(avg.Err.tot = mean(Err.tot)) %>% 
    filter(avg.Err.tot == min(avg.Err.tot))
}

clas.cv.test.errors %>% 
  ggplot(mapping = aes(x = param.k, group = test.fold_id)) +
  xlab("#Neighbours") + ylab("Error rate") +
  geom_line(aes(y = Err.tot, color = "Test Err (Total)")) + 
  geom_line(aes(y = Err.1  , color = "Test Err in class 1"), linetype = "dashed", alpha = 0.5) +
  geom_line(aes(y = Err.2  , color = "Test Err in class 2"), linetype = "dashed", alpha = 0.5) +
  geom_line(aes(y = Err.3  , color = "Test Err in class 3"), linetype = "dashed", alpha = 0.5) +
  theme_light()

clas.cv.test.errors %>% 
  group_by(param.k) %>% 
  summarise(avg.Err.tot = mean(Err.tot),
            avg.Err.1   = mean(Err.1),
            avg.Err.2   = mean(Err.2),
            avg.Err.3   = mean(Err.3)) %>% 
  ggplot(mapping = aes(x = param.k)) +
  ggtitle(paste0("Plot of Average error in ", clas.cv.k_folds, "-fold CV")) +
  xlab("#Neighbours") + ylab("Avg Error rate (Cross-Validation)") +
  geom_line(aes(y = avg.Err.tot , color = "Test Err (Total)")) + 
  geom_line(aes(y = avg.Err.1   , color = "Test Err in class 1"), linetype = "dashed", alpha = 1) +
  geom_line(aes(y = avg.Err.2   , color = "Test Err in class 2"), linetype = "dashed", alpha = 1) +
  geom_line(aes(y = avg.Err.3   , color = "Test Err in class 3"), linetype = "dashed", alpha = 1) +
  theme_light()

```

After k-fold =10, we obtained optimal results at KNN k=15 with an error rate of 0.41, The results are similar

## Model Selection for parametric methods

### Forward and backward regression

In order to use QDA, LDA, naive bayes, and logistic regression we can try to see if model selection can be useful for us.

Using linear regression, backward selection and forward selection, along with lasso regression, we found those formulas:

```{r eval=FALSE}
formula <- c(y~.)
formula <- append(formula, y~X26+X44+X47+X40+X24+X19+X16+X5) # with selection of significant parameters of linear regression
formula <- append(formula, y ~ X1 + X3 + X5 + X6 + X16 + X17 + X19 + X22 + X23 + X24 + X26 + X28 + X30 + X33 + X38 + X40 + X41 + X42 + X44 + X47) # With backward and forward selection (same model)
formula <- append(formula, y ~ X1 + X3 + X5 + X6 + X16 + X19 + X24 + X26 + X28 + X40 + X44 + X47) # with coefficients extraction of the LASSO model

```

However:

-   The linear model had a adjusted R² of 0.04436, so it can't be reliable.

-   The best predictor of the backward/forward selection had a adjusted R² of 0.099, that is to say that the model is unreliable.

-   For the lasso regression, along for the two other formulas, we always found worst error rates with all the classifications methods used.

That is to say that, although we tried classification with all the four models, the three with coefficients selection couldn't help us.

### Principal component analysis

We tried to use PCA to see if it is also relevant to reduce dimensionnality in our model.

```{r eval=FALSE}
X <- clas.set[1:50]
X<-scale(X) # Scale to improve PCA
pca<-princomp(X)
Z<-pca$scores
lambda<-pca$sdev^2
plot(cumsum(lambda)/sum(lambda),type="l",xlab="q",ylab="proportion of explained variance")
```

```{r , out.width="50%",echo=FALSE}
X <- TPN1_a22_clas_app[1:50]
X<-scale(X)
pca<-princomp(X)
Z<-pca$scores
lambda<-pca$sdev^2
plot(cumsum(lambda)/sum(lambda),type="l",xlab="q",ylab="proportion of explained variance")
```

We generally find no relevant analysis on PCA, as the curve really plummet onto 0 explained variance, so it is difficult to find a trade-off to have a good explained variance and a small number of dimensions.

## QDA

```{r message=FALSE, warning=FALSE,echo=FALSE}
source("scripts/f_clas_qda.R")

#f_clas_qda(train = clas.data.train, test = clas.data.test)

```

```{r eval=FALSE}
clas.set$y <- as.factor(clas.set$y)

fit.qda  <- qda(y ~ ., data=data.train)
pred.qda <- predict(fit.qda, newdata=data.test)
perf.qda <- table(data.test$y, pred.qda$class)
sum (diag(perf.qda)) / n_test 
err.qda <- 1-sum (diag(perf.qda)) / n_test
```

After one QDA, we obtained an error rate of 0.38, which is already better than the optimal KNN result.

We used the K-Fold Cross Validation to have a more precise result on the robustness of the QDA model.

## QDA with K-fold validation

```{r eval=FALSE}
K<-10
err = rep(0,4)
par(mfrow = c(2, 2))
for (i in (1:4)){
  CV <- rep(0,10)
  folds=sample(1:K,n_clas,replace=TRUE)
  for(k in (1:K)){
    class<-qda(formula[[i]],data=clas.set[folds!=k,])
    pred<-predict(class,newdata=clas.set[folds==k,])
    conf <- table(clas.set[folds==k,]$y, pred$class)
    CV[k]<-1-sum(diag(conf))/nrow(clas.set[folds==k,])
  }
  CV_mean<-mean(CV)
  plot(CV, type="l")
  err[i] <- CV_mean
}
err.qda = min(err) # We find the complete model (y~.) gives the best error rate
```

We found a global error rate of **0.3205** for the QDA, thanks to the 10-Folds cross validation.

## LDA

We show directly how we use the LDA methods with K-Folds Cross validation to evaluate the error rate of the

## LDA with K-fold validation

```{r eval=FALSE}
K<-10
err = rep(0,4)
par(mfrow = c(2, 2))

for (i in (1:4)){
  CV <- rep(0,10)
  folds=sample(1:K,n_clas,replace=TRUE)
  for(k in (1:K)){
    class<-lda(formula[[i]],data=clas.set[folds!=k,])
    pred<-predict(class,newdata=clas.set[folds==k,])
    conf <- table(clas.set[folds==k,]$y, pred$class)
    CV[k]<-1-sum(diag(conf))/nrow(clas.set[folds==k,])
  }
  CV_mean<-mean(CV)
  err[i] <- CV_mean
}
err.lda = min(err) # We find the complete model (y~.) gives the best error rate
```

We found an mean error rate across all 10 folds of **0.4148** using LDA, on one realization, that is bigger error rate than the one of QDA method.

## Naive Bayes

We then tried to use the QDA naive bayes method to see if it predicts better than the others.

```{r eval=FALSE}
clas.set$y <- factor(clas.set$y)

K<-10
err = rep(0,4)
par(mfrow = c(2, 2))
for (i in (1:4)){
  CV <- rep(0,10)
  folds=sample(1:K,n_clas,replace=TRUE)
  for(k in (1:K)){
    class<-naive_bayes(formula[[i]],data=clas.set[folds!=k,])
    pred<-predict(class,newdata=clas.set[folds==k,][1:50])
    conf <- table(clas.set[folds==k,]$y, pred)
    CV[k]<-1-sum(diag(conf))/nrow(clas.set[folds==k,])
  }
  CV_mean<-mean(CV)
  err[i] <- CV_mean
}
err.naiveqda = min(err)
```

This time we found, for one realization of 10-Folds cross validation for Naive Bayes method on our data set, of 0.3509.

## Multinomial logistic regression

Here, our data have the classes c \> 2, so we used the "Multinomial logistic regression" method.

```{r eval=FALSE}
K<-10
err = rep(0,4)
par(mfrow = c(2, 2))

for (i in (1:4)){
  CV <- rep(0,10)
  folds=sample(1:K,n_clas,replace=TRUE)
  for(k in (1:K)){
    class<-multinom(formula[[i]],data=clas.set[folds!=k,])
    pred<-predict(class,newdata=clas.set[folds==k,])
    conf <- table(clas.set[folds==k,]$y, pred)
    CV[k]<-1-sum(diag(conf))/nrow(clas.set[folds==k,])
    #  class<-qda(y~.,data=clas.set[folds==k,])
    #  pred<-predict(class,newdata=clas.set[folds==k,])
    #  conf <- table(clas.set[folds==k,]$y, pred$class)
    #  CV2<-CV2+1-sum(diag(conf))/nrow(clas.set[folds==k,])
  }
  CV_mean<-mean(CV)
  err[i] <- CV_mean
}
err.naiveqda = min(err)
```

This time, error rates for one realization of the K-Folds CV with the multinomial logistic regression is of \*\*0.4212\*\*.

## ROC Curves

The area under the curve (AUC) of QDA ROC Curve is really low (close to 0.5). It means that in this case, QDA cannot be reliable. The best that we find is Naive Bayes and that is also the case for the error rate, so Naive Bayes method is best to choose (we couldn't plot logistic regression with pROC library, but we already now it has a high average error rate).

## Summary for classification

As we have the lowest error rate (about 0.33) and and good ROC curve for the Naive Bayes method, we choose this method.

## 
