---
title: "R Notebook"
author: "Pengyu JI"
output:
  pdf_document:
    latex_engine: xelatex
  
---


# Régression
## Load and preparation of data : partitioning part train and test
```{r message=FALSE, warning=FALSE}
library(pls)
library(leaps)
library(dplyr)
library(FNN)
library(glmnet)


reg.set <- read.table('TPN1_a22_reg_app.txt', header = TRUE)
train.percentage <- 2/3
n_reg <- nrow(reg.set)
n_train <- as.integer(n_reg * train.percentage)
n_test <- n_reg - n_train
set.seed(69)
id_train <- sample(n_reg, n_train)
data.train <- reg.set[id_train,]
data.test <- reg.set[-id_train,]
y.test <- reg.set[-id_train, c(101)]
y.train <- reg.set[id_train, c(101)]

```

## Exploration of data
```{r out.width="50%"}
boxplot(as.data.frame(reg.set[1:100]))
```

We can see that the range of all the variables are contained between 0 and 10

## PCA
```{r out.width="50%"}
pcr_model <- pcr(y~., data = data.test, validation = "CV")
validationplot(pcr_model, val.type="MSEP")
```
Looking at the graph, we can conclude that all the variables are needed to have an optimal performance, so this method is rejected.

## Linear model
```{r}
reg.set.lm <- lm(formula = y ~., data = data.train)
reg.set.lm.summary <- summary(reg.set.lm)
```
From the summary, the general linear model has already a very little p-value and an optimal R-squared: 0.9626.

```{r}
pre.lm <- predict(reg.set.lm, newdata = data.test)
mse.lm <- mean((pre.lm - y.test) ^ 2)#200.176
```
The linear model gives us an MSE of 200.176

## Regression using the significant variables
```{r include=FALSE}
x.app.lm <- data.train[c(6, 11, 12, 15, 17,22, 23, 25, 27, 32, 33, 35, 37, 39, 46, 47, 48, 49, 52, 54, 56, 59, 60, 63, 68, 70, 72, 74, 79, 83, 84, 87, 88, 89, 90, 91, 96), ] 
data.test.lm <- data.test[c(6, 11, 12, 15, 17, 22, 23, 25, 27, 32, 33, 35, 37, 39, 46, 47, 48, 49, 52, 54, 56, 59, 60, 63, 68, 70, 72, 74, 79, 83, 84, 87, 88, 89, 90, 91, 96), ]

```

```{r warning=FALSE}
reg.set.lm.revise <- lm(formula = y~. , data = x.app.lm)
pre.lm.revise <- predict(reg.set.lm.revise, newdata = data.test.lm)
mse.lm1 <- mean((pre.lm - y.test) ^ 2)#200.176
```

The MSE is still 200.176.

## KNN using the significant variables
Dans cette partie, on essaie de faire la méthode KNN avec les variables précedemment établies
```{r include=FALSE}
x.app_k <- data.train[c(6, 11, 12, 15, 17, 22, 23, 25, 27, 32, 33, 35, 37, 39, 46, 47, 48, 49, 52, 54, 56, 59, 60, 63, 68, 70, 72, 74, 79, 83, 84, 87, 88, 89, 90, 91, 96), c(-101)] 
y.app_k <- data.train[, c(101)]
x.test_k <- data.test[, c(-101)]
x.app_k.scale <- scale(data.train[c(6, 11, 12, 15, 17, 22, 23, 25, 27, 32, 33, 35, 37, 39, 46, 47, 48, 49, 52, 54, 56, 59, 60, 63, 68, 70, 72, 74, 79, 83, 84, 87, 88, 89, 90, 91, 96), c(-101)] )
x.test_k.scale <- scale(data.test[, c(-101)])

```

### Data with/without scaling
```{r out.width="50%"}
knn_k_max <- 37

tmp <- sapply(1:knn_k_max, function(local_k){
  reg.knn.noscale <- knn.reg(train = x.app_k, test = x.test_k, y = y.app_k, k = local_k)
  res <- mean((reg.knn.noscale$pred - y.test) ^ 2)
  return(res)
})

tmp.scale <- sapply(1:knn_k_max, function(local_k){
  reg.knn.scale <- knn.reg(train = x.app_k.scale, test = x.test_k.scale, y = y.app_k, k = local_k)
  res.scale <- mean((reg.knn.scale$pred - y.test) ^ 2)
  return(res.scale)
})

knn_MSE_noscale <- tmp
knn_MSE_scale <- tmp.scale
#erreur global
plot(1:knn_k_max, knn_MSE_noscale, 
     type='b', col='blue',
     ylim=range(knn_MSE_scale),
     xlab='k', ylab='MSE', lty = 1, pch = 1)

lines(1:knn_k_max, knn_MSE_scale, type='b', col='red', lty = 1, pch = 1)
```


```{r echo=FALSE, out.width="50%"}
data.frame("which.min(knn_MSE_scale)" = which.min(knn_MSE_scale),
           "min(knn_MSE_scale)" = min(knn_MSE_scale),
           "which.min(knn_MSE_noscale)" = which.min(knn_MSE_noscale),
           "min(knn_MSE_noscale)" = min(knn_MSE_noscale))
```

The minimum MSE is 3707.

## Subset selection
### Best subset
Due to the fact that there is too many variables, we can't use the best subset selection.

### Forward selection

To do this, we use regsubsets to find the significant variables with which we can reach a best adjr2.
```{r}
reg.selection.forward <- regsubsets(y~., data = data.train, method = "forward", nbest = 1, nvmax = 100)
summary_forward <- summary(reg.selection.forward)
```
From summary_forward, we can get a list of potential models. To choose which is best suited, we compare the adjusted determination coefficient of each model. This value gives us the possibility to measure the quality of the linear regression.

```{r}
rss<-data.frame(summary_forward$outmat, RSS=summary_forward$rss)
rsquare_max_forward <- summary_forward$outmat[which.max(summary_forward$adjr2),]
#La ligne avec la plus grande adjr2
rsquare_max_forward[rsquare_max_forward == '*'] <- as.numeric(1)
rsquare_max_forward[rsquare_max_forward == ' '] <- as.numeric(0)
rsquare_max_forward <- as.numeric(rsquare_max_forward)
#Le masque pour sélectionner les variables
reg.subset.forward <- reg.set[c(rsquare_max_forward==1)]
```
rsquare_max_forward is therefore a vector containing integers between 0 and 1 (1 for the columns to keep in the model and 0 for the columns to discard). We filter the data with it.

```{r}
n.subset.forward <- nrow(reg.subset.forward)
set.seed(69)
n.subset.forward.train <- as.integer(train.percentage * n.subset.forward)
n.subset.forward.sample <- sample(n.subset.forward, n.subset.forward.train)
reg.subset.forward.train <- reg.subset.forward[n.subset.forward.sample,]
reg.subset.forward.test <- reg.subset.forward[-n.subset.forward.sample,]

```

```{r}
reg.subset.forward.lm <- lm(formula = y~., data = reg.subset.forward.train)
reg.subset.forward.lm.predict <- predict(reg.subset.forward.lm, newdata = reg.subset.forward.test)
reg.subset.forward.mse <- mean((reg.subset.forward.lm.predict - reg.subset.forward.test$y) ^ 2)#188.44
```
The MSE is 188.44.

### Backward selection
We use the same method previously mentioned to find the variables.
```{r include=FALSE}
reg.selection.backward <- regsubsets(y~., data = data.train, method = "backward", nbest = 1, nvmax = 100)
summary_backward <- summary(reg.selection.backward)
plot(reg.selection.backward, scale = "adjr2")
#rss<-data.frame(summary_forward$outmat, RSS=summary_forward$rss)
rsquare_max_backward <- summary_backward$outmat[which.max(summary_backward$adjr2),]
rsquare_max_backward[rsquare_max_backward == '*'] <- as.numeric(1)
rsquare_max_backward[rsquare_max_backward == ' '] <- as.numeric(0)
rsquare_max_backward <- as.numeric(rsquare_max_backward)
reg.subset.backward <- reg.set[c(rsquare_max_backward==1)]
```
```{r}
n.subset.backward <- nrow(reg.subset.backward)
set.seed(69)
n.subset.backward.train <- as.integer(train.percentage * n.subset.backward)
n.subset.backward.sample <- sample(n.subset.backward, n.subset.backward.train)
reg.subset.backward.train <- reg.subset.backward[n.subset.backward.sample,]
reg.subset.backward.test <- reg.subset.backward[-n.subset.backward.sample,]
reg.subset.backward.lm <- lm(formula = y~., data = reg.subset.backward.train)
reg.subset.backward.lm.predict <- predict(reg.subset.backward.lm, newdata = reg.subset.backward.test)
reg.subset.backward.mse <- mean((reg.subset.backward.lm.predict - reg.subset.backward.test$y) ^ 2)#186.92
```
The MSE is 186.92.

### K-Cross validation de Backward selection
To validate the model we have chosen, we have a K-Fold cross-validation. We will compare the different values obtained using the method with other potential models.
```{r include=FALSE}
Formula <- c(y~.-X2, 
             y~.-X2-X5, 
             y~.-X2-X5-X7, 
             y~.-X2-X5-X7-X8,
             y~.-X2-X5-X7-X8-X9,
             y~.-X2-X5-X7-X8-X9-X10, 
             y~.-X2-X5-X7-X8-X9-X10-X16, 
             y~.-X2-X5-X7-X8-X9-X10-X16-X18,
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19, 
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19-X20, 
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19-X20-X28,
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19-X20-X28-X30,
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19-X20-X28-X30-X34,
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19-X20-X28-X30-X34-X38,
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19-X20-X28-X30-X34-X38-X40,
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19-X20-X28-X30-X34-X38-X40-X41,
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19-X20-X28-X30-X34-X38-X40-X41-X44,
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19-X20-X28-X30-X34-X38-X40-X41-X44-X53,
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19-X20-X28-X30-X34-X38-X40-X41-X44-X53-X55,
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19-X20-X28-X30-X34-X38-X40-X41-X44-X53-X55-X57,
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19-X20-X28-X30-X34-X38-X40-X41-X44-X53-X55-X57-X61,
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19-X20-X28-X30-X34-X38-X40-X41-X44-X53-X55-X57-X61-X64,
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19-X20-X28-X30-X34-X38-X40-X41-X44-X53-X55-X57-X61-X64-X65,
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19-X20-X28-X30-X34-X38-X40-X41-X44-X53-X55-X57-X61-X64-X65-X66,
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19-X20-X28-X30-X34-X38-X40-X41-X44-X53-X55-X57-X61-X64-X65-X66-X67,
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19-X20-X28-X30-X34-X38-X40-X41-X44-X53-X55-X57-X61-X64-X65-X66-X67-X73,
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19-X20-X28-X30-X34-X38-X40-X41-X44-X53-X55-X57-X61-X64-X65-X66-X67-X73-X76,
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19-X20-X28-X30-X34-X38-X40-X41-X44-X53-X55-X57-X61-X64-X65-X66-X67-X73-X76-X77,
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19-X20-X28-X30-X34-X38-X40-X41-X44-X53-X55-X57-X61-X64-X65-X66-X67-X73-X76-X77-X78,
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19-X20-X28-X30-X34-X38-X40-X41-X44-X53-X55-X57-X61-X64-X65-X66-X67-X73-X76-X77-X78-X79,
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19-X20-X28-X30-X34-X38-X40-X41-X44-X53-X55-X57-X61-X64-X65-X66-X67-X73-X76-X77-X78-X79-X81,
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19-X20-X28-X30-X34-X38-X40-X41-X44-X53-X55-X57-X61-X64-X65-X66-X67-X73-X76-X77-X78-X79-X81-X85,
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19-X20-X28-X30-X34-X38-X40-X41-X44-X53-X55-X57-X61-X64-X65-X66-X67-X73-X76-X77-X78-X79-X81-X85-X93,
             y~.-X2-X5-X7-X8-X9-X10-X16-X18-X19-X20-X28-X30-X34-X38-X40-X41-X44-X53-X55-X57-X61-X64-X65-X66-X67-X73-X76-X77-X78-X79-X81-X85-X93-X97
)

```

```{r}
K <- 10
fold <- sample(K, n_train, replace = TRUE)
CV <- rep(0, 10)
for (i in (1:10)){
  for (k in (1:K)){
    reg.cross<-lm(Formula[[i]],data=reg.set[fold!=k,])
    pred.cross <- predict(reg.cross, newdata=reg.set[fold == k,])
    CV[i]<-CV[i]+ sum((reg.set$y[fold==k]-pred.cross)^2)
  }
  CV[i]<-CV[i] / n_reg
}
CV.min = min(CV)#181

```
We obtain the lowest value for the model we had chosen (181).

## Ridge Regression
```{r include=FALSE }
x<-model.matrix(y~.,reg.set)
y<-reg.set$y
data.train.regu <- x[id_train,]
y.train.regu <- y[id_train]
data.test.regu <- x[-id_train,]
y.test.regu <- y[-id_train]

```

```{r out.width="50%"}
cv.out.ridge <- cv.glmnet(data.train.regu, y.train.regu, alpha = 0)
#plot(cv.out.ridge)
fit.ridge <- glmnet(data.train.regu, y.train.regu, lambda = cv.out.ridge$lambda.min, alpha = 0)
ridge.predict <- predict(fit.ridge, s = cv.out.ridge$lambda.min, newx = data.test.regu)
mse.ridge <- mean((ridge.predict - y.test.regu) ^ 2)#200.44
```
With this method, we obtain an MSE of 200.44.

## Lasso regression
```{r out.width="50%"}
cv.out.lasso <- cv.glmnet(data.train.regu, y.train.regu, alpha = 1)
#plot(cv.out.lasso)
fit.lasso <- glmnet(data.train.regu, y.train.regu, lambda = cv.out.lasso$lambda.min, alpha = 1)
lasso.predict <- predict(fit.lasso, s = cv.out.lasso$lambda.min, newx = data.test.regu)
mse.lasso <- mean((lasso.predict - y.test.regu) ^ 2)#178
```
With this method, we obtain an MSE of 178.13.

# Conclusion
```{r include=FALSE}
mse.knn.scale <- min(knn_MSE_scale)
mse.knn.noscale <- min(knn_MSE_noscale)
```

```{r}
cbind(mse.lm, mse.lm1, mse.knn.scale, mse.knn.noscale, reg.subset.forward.mse, reg.subset.backward.mse, CV.min, mse.ridge, mse.lasso)
```

